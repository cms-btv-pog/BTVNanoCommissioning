import psutil
import os
from tqdm import tqdm
import uproot
import warnings
import argparse
import glob
import matplotlib.pyplot as plt
from concurrent.futures import ThreadPoolExecutor, as_completed
import pandas as pd
import numpy as np
import seaborn as sns
import awkward as ak
from BTVNanoCommissioning.helpers.definitions import definitions_dict
from BTVNanoCommissioning.helpers.definitions import disc_list


# Suppress the specific FutureWarning from uproot
warnings.filterwarnings("ignore", category=FutureWarning, module="uproot")

filtered_names = [
    name for name in disc_list if name.endswith("B") and not name.endswith("CvB")
]


def is_within_range(array, min_value, max_value):
    """Check if all values in the array are within the specified range."""
    return ak.all((array >= min_value) & (array <= max_value))


def load_single_file(
    file_path,
    base_dir,
    chunk_size=100000,
    temp_subdir="temp_data",
    limit_inputs=False,
    limit_outputs=False,
    SMu=False,
    flavour_split=False,
):
    temp_dir = os.path.join(base_dir, temp_subdir)
    if not os.path.exists(temp_dir):
        os.makedirs(temp_dir, exist_ok=True)

    with uproot.open(file_path) as file:
        if "Events" not in file:
            print(f"Skipping file without 'Events' key: {file_path}")
            return

        tree = file["Events"]
        if tree.num_entries == 0:
            print(f"Skipping empty file: {file_path}")
            return

        # Apply limit_inputs filter
        if limit_inputs:
            filtered_definitions_dict = {
                k: v
                for k, v in definitions_dict.items()
                if any(sub in k for sub in ["Npfcan", "Cpfcan", "sv"])
                and k.endswith("_0")
                or not any(sub in k for sub in ["Npfcan", "Cpfcan", "sv"])
            }
        else:
            filtered_definitions_dict = definitions_dict

        # Initialize filtered_names
        filtered_names = [
            name
            for name in disc_list
            if name.endswith("B") and not name.endswith("CvB")
        ]

        # Apply limit_outputs filter
        if limit_outputs:
            filtered_names = [
                name
                for name in filtered_names
                if name.endswith("B")
                and "Neg" not in name
                and "btagPNetProbB" not in name
            ]

        # Filter branches to include only those with 'SelJets' in their name, containing keys from filtered_definitions_dict, and matching filtered_names
        branches = [
            branch
            for branch in tree.keys()
            if "MuonJet" in branch
            and (
                any(key in branch for key in filtered_definitions_dict.keys())
                or any(name in branch for name in filtered_names)
            )
        ]

        if limit_outputs:
            branches = [
                branch
                for branch in branches
                if not any(name in branch for name in filtered_names)
                or any(
                    name in branch for name in filtered_names if branch.endswith("B")
                )
            ]

        # Include branches that have 'SMu' in their name if the SMu flag is True
        if SMu:
            smu_branches = [branch for branch in tree.keys() if "SoftMuon" in branch]
            smu_branches.append("MuonJet_muEF")
            if limit_inputs:
                smu_branches = [
                    "SoftMuon_tunepRelPt",
                    "SoftMuon_pfRelIso03_chg",
                    "SoftMuon_eta",
                    "SoftMuon_phi",
                    "SoftMuon_jetPtRelv2",
                    "SoftMuon_dxy",
                    "SoftMuon_dxyErr",
                    "SoftMuon_jetRelIso",
                    "SoftMuon_sip3d",
                    "SoftMuon_dzErr",
                    "SoftMuon_pfRelIso04_all",
                    "SoftMuon_ip3d",
                    "SoftMuon_pt",
                    "SoftMuon_ptErr",
                    "SoftMuon_tkRelIso",
                    "SoftMuon_dz",
                    "SoftMuon_pfRelIso03_all",
                    "MuonJet_muEF",
                ]
            branches.extend(smu_branches)
            branches = list(set(branches))  # Remove duplicates

        # Include the flavour column if flavour_split is enabled
        if flavour_split:
            branches.append("MuonJet_hadronFlavour")

        # Extract manual ranges for x and y columns if they don't start with SelJet_ or SMu_
        x_limits_dict = {}
        for x_col in branches:
            if not x_col.startswith(("SelJet_", "SoftMuon_", "MuonJet_")):
                x_limits = definitions_dict.get(x_col, {}).get("manual_ranges", None)
            else:
                x_col_str = (
                    x_col.lstrip("SelJet_").lstrip("SoftMuon_").lstrip("MuonJet_")
                )
                x_limits = definitions_dict.get(x_col_str, {}).get(
                    "manual_ranges", None
                )

            x_limits_dict[x_col] = x_limits

        for i, data_chunk in enumerate(
            tree.iterate(branches, library="pd", step_size=chunk_size)
        ):
            if isinstance(data_chunk, tuple):
                data_chunk = data_chunk[0]  # Extract the DataFrame from the tuple

            # Filter the data based on MuJet_Cpfcan_ptrel_0
            ###data_chunk = data_chunk[data_chunk['MuJet_DeepJet_Cpfcan_ptrel_0'] >= -20]

            # Filter the data based on x_limits
            for col, limits in x_limits_dict.items():
                if limits is not None:
                    min_value, max_value = limits
                    data_chunk = data_chunk[
                        (data_chunk[col] >= min_value) & (data_chunk[col] <= max_value)
                    ]
            temp_file_path = os.path.join(
                temp_dir, f"{os.path.basename(file_path)}_chunk_{i}.parquet"
            )
            data_chunk.to_parquet(temp_file_path)


def inspect_first_file(
    file_path, limit_inputs=False, limit_outputs=False, SMu=False, flavour_split=False
):
    with uproot.open(file_path) as file:
        print(f"Inspecting file: {file_path}")
        print("Keys:", file.keys())
        if "Events" in file:
            tree = file["Events"]
            print("Number of entries in 'Events':", tree.num_entries)

            # Apply limit_inputs filter
            if limit_inputs:
                filtered_definitions_dict = {
                    k: v
                    for k, v in definitions_dict.items()
                    if any(sub in k for sub in ["Npfcan", "Cpfcan", "sv"])
                    and k.endswith("_0")
                    or not any(sub in k for sub in ["Npfcan", "Cpfcan", "sv"])
                }
            else:
                filtered_definitions_dict = definitions_dict

            # Initialize filtered_names
            filtered_names = [
                name
                for name in disc_list
                if name.endswith("B") and not name.endswith("CvB")
            ]

            # Apply limit_outputs filter
            if limit_outputs:
                filtered_names = [
                    name
                    for name in filtered_names
                    if name.endswith("B")
                    and "Neg" not in name
                    and "btagPNetProbB" not in name
                ]

            print(tree.keys())
            # Filter branches to include only those with 'SelJets' in their name, containing keys from definitions_dict, and matching filtered_names

            branches = [
                branch
                for branch in tree.keys()
                if "MuonJet" in branch
                and (
                    any(key in branch for key in filtered_definitions_dict.keys())
                    or any(name in branch for name in filtered_names)
                )
            ]
            if limit_outputs:
                branches = [
                    branch
                    for branch in branches
                    if not any(name in branch for name in filtered_names)
                    or any(
                        name in branch
                        for name in filtered_names
                        if branch.endswith(name)
                    )
                ]
            print("Filtered Branches:", branches)
            print(len(branches))
            if SMu:
                smu_branches = [
                    branch for branch in tree.keys() if "SoftMuon" in branch
                ]
                smu_branches.append("MuonJet_muEF")
                print(smu_branches)
                if limit_inputs:
                    smu_branches = [
                        "SoftMuon_tunepRelPt",
                        "SoftMuon_pfRelIso03_chg",
                        "SoftMuon_eta",
                        "SoftMuon_phi",
                        "SoftMuon_jetPtRelv2",
                        "SoftMuon_dxy",
                        "SoftMuon_dxyErr",
                        "SoftMuon_jetRelIso",
                        "SoftMuon_sip3d",
                        "SoftMuon_dzErr",
                        "SoftMuon_pfRelIso04_all",
                        "SoftMuon_ip3d",
                        "SoftMuon_pt",
                        "SoftMuon_ptErr",
                        "SoftMuon_tkRelIso",
                        "SoftMuon_dz",
                        "SoftMuon_pfRelIso03_all",
                        "MuonJet_muEF",
                    ]

                branches.extend(smu_branches)
                branches = list(set(branches))  # Remove duplicates
                print("Branches with SMu:", smu_branches)

            # Include the flavour column if flavour_split is enabled
            if flavour_split:
                branches.append("MuonJet_hadronFlavour")

            print("Branches:", branches)

            # Find branches that contain any of the keys from definitions_dict
            matching_keys = [
                branch
                for branch in branches
                if any(key in branch for key in definitions_dict.keys())
            ]
            print("Branches containing keys from definitions_dict:", matching_keys)
            print(len(matching_keys))

            matching_keys = [
                branch
                for branch in branches
                if any(name in branch for name in filtered_names)
            ]
            print("Branches containing keys from definitions_dict:", matching_keys)
            print(len(matching_keys))

            # Print the filtered names
            print("Filtered Names:", filtered_names)

            # Extract manual ranges for x and y columns if they don't start with SelJet_ or SMu_
            x_limits_dict = {}
            for x_col in branches:
                if not x_col.startswith(("SelJet_", "SoftMuon_", "MuonJet_")):
                    x_limits = definitions_dict.get(x_col, {}).get(
                        "manual_ranges", None
                    )
                else:
                    x_col_str = (
                        x_col.lstrip("SelJet_").lstrip("SoftMuon_").lstrip("MuonJet_")
                    )
                    x_limits = definitions_dict.get(x_col_str, {}).get(
                        "manual_ranges", None
                    )

                x_limits_dict[x_col] = x_limits
                # Debugging prints
                # print(f"x_col: {x_col}")
                # print(f"Manual ranges for {x_col}: {x_limits}")

            # Load the first chunk to inspect memory usage
            data_chunk = next(tree.iterate(branches, library="pd", step_size=100000))
            if isinstance(data_chunk, tuple):
                data_chunk = data_chunk[0]  # Extract the DataFrame from the tuple

            # Filter the data based on x_limits
            for col, limits in x_limits_dict.items():
                if limits is not None:
                    min_value, max_value = limits
                    data_chunk = data_chunk[
                        (data_chunk[col] >= min_value) & (data_chunk[col] <= max_value)
                    ]

            memory_usage = data_chunk.memory_usage(deep=True).sum()
            print(
                f"Memory usage of the first DataFrame chunk: {memory_usage / (1024 ** 2):.2f} MB"
            )
        else:
            print("'Events' key not found in the file.")


def load_data(
    file_paths,
    base_dir,
    batch_size=2,
    temp_subdir="temp_data",
    output_subdir="parquet_data",
    max_files=None,
    limit_inputs=False,
    limit_outputs=False,
    SMu=False,
    flavour_split=False,
    specify_MC=False,
):
    # Step 1: Inspect the nominal folder
    nominal_folder = os.path.join(base_dir, "nominal")
    nominal_subfolders = {}
    if os.path.exists(nominal_folder):
        for subfolder in os.listdir(nominal_folder):
            subfolder_path = os.path.join(nominal_folder, subfolder)
            if os.path.isdir(subfolder_path):
                # Count the number of .root files in the subfolder
                root_files = [
                    f for f in os.listdir(subfolder_path) if f.endswith(".root")
                ]
                root_file_count = len(root_files)
                # Print the count of .root files
                print(
                    f"Subfolder: {subfolder}, Number of .root files: {root_file_count}"
                )
                # Store the subfolder path for later use
                nominal_subfolders[subfolder] = subfolder_path

    if max_files is not None:
        file_paths = file_paths[:max_files]

    print(base_dir)
    if "MC" in base_dir:
        print("MC file detected")

    # Check if the base directory name contains "MC" and the specify_MC flag is active
    if "MC" in base_dir and specify_MC:
        # Filter file_paths to only include files from a specific subfolder
        print("Filtering file paths to include files from a specific subfolder")
        # specific_subfolder = "QCD_PT-50to80_MuEnrichedPt5_TuneCP5_13p6TeV_pythia8"  # Replace with the actual subfolder name
        specific_subfolder = "QCD_PT-80to120_TuneCP5_13p6TeV_pythia8"
        file_paths = [fp for fp in file_paths if specific_subfolder in fp]

    output_dir = os.path.join(base_dir, output_subdir)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    with ThreadPoolExecutor(max_workers=batch_size) as executor:
        futures = [
            executor.submit(
                load_single_file,
                file_path,
                base_dir,
                temp_subdir=temp_subdir,
                limit_inputs=limit_inputs,
                limit_outputs=limit_outputs,
                SMu=SMu,
                flavour_split=flavour_split,
            )
            for file_path in file_paths
        ]
        for future in tqdm(
            as_completed(futures), total=len(futures), desc="Processing files"
        ):
            future.result()  # Ensure any exceptions are raised

    combined_data = {}
    temp_files = glob.glob(os.path.join(base_dir, temp_subdir, "*.parquet"))
    print(f"Temporary files found: {len(temp_files)}")  # Debugging line
    event_counts = []
    for temp_file in temp_files:
        df = pd.read_parquet(temp_file)
        key = os.path.basename(temp_file).replace(".parquet", "")
        combined_data[key] = df  # Store the DataFrame directly
        event_counts.append((key, len(df)))  # Store the number of events and the key

    # Define ranking factors for each subfolder
    ### FIXME: sumw has to change, whenever you run a different set. Pay attention!!!
    ### FIXME: The former factor is the xs, the second - sumw!
    ranking_factors = {
        "QCD_PT-15to20_MuEnrichedPt5_TuneCP5_13p6TeV_pythia8": 295600 / 142083,
        "QCD_PT-20to30_MuEnrichedPt5_TuneCP5_13p6TeV_pythia8": 2689000 / 5926,
        "QCD_PT-30to50_MuEnrichedPt5_TuneCP5_13p6TeV_pythia8": 1442000 / 339153,
        "QCD_PT-50to80_MuEnrichedPt5_TuneCP5_13p6TeV_pythia8": 405800 / 94942,
        "QCD_PT-80to120_MuEnrichedPt5_TuneCP5_13p6TeV_pythia8": 96060 / 488561,
        "QCD_PT-120to170_MuEnrichedPt5_TuneCP5_13p6TeV_pythia8": 23230 / 210233,
        "QCD_PT-1700to300_MuEnrichedPt5_TuneCP5_13p6TeV_pythia8": 7763 / 216617,
        "QCD_PT-300to470_MuEnrichedPt5_TuneCP5_13p6TeV_pythia8": 701.4 / 196409,
        "QCD_PT-470to600_MuEnrichedPt5_TuneCP5_13p6TeV_pythia8": 68.24 / 254794,
        "QCD_PT-600to800_MuEnrichedPt5_TuneCP5_13p6TeV_pythia8": 21.23 / 34096,
        "QCD_PT-800to1000_MuEnrichedPt5_TuneCP5_13p6TeV_pythia8": 3.9 / 155007,
        "QCD_PT-1000_MuEnrichedPt5_TuneCP5_13p6TeV_pythia8": 1.323 / 237819,
        "QCD_PT-50to80_TuneCP5_13p6TeV_pythia8": 16760000.0 / 306000,
        "QCD_PT-80to120_TuneCP5_13p6TeV_pythia8": 2514000.0 / 362000,
        "QCD_PT-120to170_TuneCP5_13p6TeV_pythia8": 442300.0 / 352000,
        "QCD_PT-170to300_TuneCP5_13p6TeV_pythia8": 113400.0 / 334000,
        "QCD_PT-300to470_TuneCP5_13p6TeV_pythia8": 7610.0 / 128000,
        "QCD_PT-470to600_TuneCP5_13p6TeV_pythia8": 626.6 / 223000,
        "QCD_PT-600to800_TuneCP5_13p6TeV_pythia8": 179.5 / 230000,
        "QCD_PT-800to1000_TuneCP5_13p6TeV_pythia8": 30.69 / 38000,
        "QCD_PT-1000to1400_TuneCP5_13p6TeV_pythia8": 8.956 / 143000,
        "QCD_PT-1400to1800_TuneCP5_13p6TeV_pythia8": 0.8112 / 219000,
        "QCD_PT-1800to2400_TuneCP5_13p6TeV_pythia8": 0.1152 / 211000,
        "QCD_PT-2400to3200_TuneCP5_13p6TeV_pythia8": 0.007574 / 334000,
        "QCD_PT-3200_TuneCP5_13p6TeV_pythia8": 0.0002313 / 44000,
    }

    # Sort event counts by the number of events in descending order
    event_counts.sort(key=lambda x: x[1], reverse=True)

    # Display the ranking
    print("Ranking by number of events:")
    for rank, (key, count) in enumerate(event_counts, start=1):
        # Strip the _chunk_0 suffix
        stripped_key = key.replace("_chunk_0", "")
        # Determine the parent subfolder
        parent_subfolder = None
        for subfolder, path in nominal_subfolders.items():
            if stripped_key in os.listdir(path):
                parent_subfolder = subfolder
                break
        print(
            f"{rank}. File: {key}, Number of events: {count}, Parent subfolder: {parent_subfolder}"
        )

    # Calculate weighted event counts
    weighted_event_counts = []
    for key, count in event_counts:
        stripped_key = key.replace("_chunk_0", "")
        parent_subfolder = None
        for subfolder, path in nominal_subfolders.items():
            if stripped_key in os.listdir(path):
                parent_subfolder = subfolder
                break
        factor = ranking_factors.get(
            parent_subfolder, 1.0
        )  # Default factor is 1.0 if not specified
        weighted_count = count * factor
        weighted_event_counts.append((key, weighted_count, parent_subfolder))

    # Sort weighted event counts by the weighted number of events in descending order
    weighted_event_counts.sort(key=lambda x: x[1], reverse=True)

    # Display the ranking
    ### Ranking done by the yields: from the highest to the lowest
    print("Ranking by weighted number of events:")
    for rank, (key, weighted_count, parent_subfolder) in enumerate(
        weighted_event_counts, start=1
    ):
        print(
            f"{rank}. File: {key}, Weighted number of events: {weighted_count}, Parent subfolder: {parent_subfolder}"
        )

    print(f"Combined data keys: {list(combined_data.keys())}")  # Debugging line
    return combined_data


def compute_correlations(data, SMu=False, flavour_split=False, split_region_B=False):
    if not data:
        raise ValueError("No data available to compute correlations.")

    # Combine all data into a single DataFrame
    combined_df = pd.concat(data.values(), axis=0)
    print(f"Combined DataFrame shape: {combined_df.shape}")  # Debugging line
    print(f"Combined DataFrame columns: {combined_df.columns}")  # Debugging line

    combined_df = combined_df[sorted(combined_df.columns)]
    print(f"Sorted DataFrame columns: {combined_df.columns}")  # Debugging line

    # Separate columns into two groups
    filtered_columns = [
        col
        for col in combined_df.columns
        if any(name in col for name in filtered_names)
    ]
    definition_columns = [
        col
        for col in combined_df.columns
        if any(key in col for key in definitions_dict.keys())
    ]

    # Include SMu columns if the SMu flag is True
    if SMu:
        smu_columns = [col for col in combined_df.columns if "SoftMuon" in col]
        smu_columns.append("MuonJet_muEF")  # Ensure MuJet_muEF is included
        definition_columns.extend(smu_columns)
        definition_columns = list(set(definition_columns))  # Remove duplicates

        # Ensure SMu columns are at the end of the definition_columns list
        smu_columns = [col for col in definition_columns if "SoftMuon" in col]
        non_smu_columns = [col for col in definition_columns if "SoftMuon" not in col]
        definition_columns = non_smu_columns + smu_columns

    # Compute the full correlation matrix
    all_columns = sorted(filtered_columns + definition_columns)
    if flavour_split:
        flavour_column = "MuonJet_hadronFlavour"
        if flavour_column not in combined_df.columns:
            raise ValueError(f"Column '{flavour_column}' not found in data.")

        flavour_groups = {
            "light": combined_df[combined_df[flavour_column] == 0],
            "charm": combined_df[combined_df[flavour_column] == 4],
            "bottom": combined_df[combined_df[flavour_column] == 5],
        }

        correlation_matrices = {}
        for flavour, df in flavour_groups.items():
            correlation_matrices[flavour] = df[all_columns].corr()

        return correlation_matrices, filtered_columns, definition_columns
    elif split_region_B:
        deepflavb_column = "MuonJet_btagDeepFlavB"
        if deepflavb_column not in combined_df.columns:
            raise ValueError(f"Column '{deepflavb_column}' not found in data.")

        # Split the DataFrame based on DeepFlavB
        high_probB_df = combined_df[combined_df[deepflavb_column] > 0.5]
        low_probB_df = combined_df[combined_df[deepflavb_column] <= 0.5]

        # Calculate correlation matrices for high and low DeepFlavB
        high_probB_corr_matrix = high_probB_df[all_columns].corr()
        low_probB_corr_matrix = low_probB_df[all_columns].corr()

        # Return the correlation matrices along with the filtered and definition columns
        return (
            {"high_probB": high_probB_corr_matrix, "low_probB": low_probB_corr_matrix},
            filtered_columns,
            definition_columns,
        )
    else:
        correlation_matrix = combined_df[all_columns].corr()
        return correlation_matrix, filtered_columns, definition_columns


# Function to print keys of definitions_dict
def print_definitions_keys():
    keys = definitions_dict.keys()
    print("Keys in definitions_dict:", keys)
    print(len(keys))


def create_label_mapping(columns):
    sorted_columns = sorted(columns)
    return {col: f"Label {i+1}" for i, col in enumerate(sorted_columns)}


def plot_correlations(
    correlation_data,
    filtered_columns,
    definition_columns,
    output_dir,
    flavour_split=False,
    split_region_B=False,
    threshold=0.5,
):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)

    def plot_heatmap(matrix, title, filename, label_mapping, threshold):
        num_labels = len(matrix.columns)
        font_size = (
            10 if num_labels < 30 else 6
        )  # Adjust font size based on number of labels

        plt.figure(figsize=(24, 16))  # Increase the size of the plot
        sns.heatmap(
            matrix,
            annot=False,
            cmap="coolwarm",
            fmt=".2f",
            cbar_kws={"label": "Correlation Coefficient"},
        )

        # Add annotations manually
        for i in range(matrix.shape[0]):
            for j in range(matrix.shape[1]):
                value = matrix.iloc[i, j]
                if abs(value) >= threshold:
                    plt.text(
                        j + 0.5,
                        i + 0.5,
                        f"{value:.2f}",
                        ha="center",
                        va="center",
                        fontsize=font_size,
                    )

        plt.title(title)

        # Replace x and y labels with numbers or symbols
        ax = plt.gca()
        ax.set_xticklabels(
            [
                label_mapping.get(label.get_text(), label.get_text())
                for label in ax.get_xticklabels()
            ]
        )
        ax.set_yticklabels(
            [
                label_mapping.get(label.get_text(), label.get_text())
                for label in ax.get_yticklabels()
            ]
        )

        # Add legend to the right of the colorbar
        colorbar = ax.collections[0].colorbar
        colorbar.ax.set_position([0.85, 0.1, 0.03, 0.8])  # Adjust position as needed

        # Create legend handles and sort them alphabetically
        sorted_label_mapping = dict(sorted(label_mapping.items()))
        legend_handles = [
            plt.Line2D([0], [0], color="w", label=f"{v}: {k}")
            for k, v in sorted_label_mapping.items()
        ]

        # Determine the number of columns for the legend
        ncol = 2 if len(legend_handles) > 20 else 1

        # Add legend
        plt.legend(
            handles=legend_handles,
            loc="center left",
            bbox_to_anchor=(1.2, 0.5),
            title="Legend",
            ncol=ncol,
        )

        plt.tight_layout()
        plt.savefig(filename, bbox_inches="tight")
        plt.close()

    # Create label mappings
    filtered_label_mapping = create_label_mapping(filtered_columns)
    definition_label_mapping = create_label_mapping(definition_columns)

    # Sort columns and index alphabetically
    sorted_filtered_columns = sorted(filtered_columns)
    sorted_definition_columns = sorted(definition_columns)

    if flavour_split:
        flavour_output_dir = os.path.join(output_dir, "flavour_splitting")
        if not os.path.exists(flavour_output_dir):
            os.makedirs(flavour_output_dir, exist_ok=True)

        for flavour, correlation_matrix in correlation_data.items():
            flavour_dir = os.path.join(flavour_output_dir, flavour)
            if not os.path.exists(flavour_dir):
                os.makedirs(flavour_dir, exist_ok=True)

            sorted_correlation_matrix = correlation_matrix.loc[
                sorted_filtered_columns + sorted_definition_columns,
                sorted_filtered_columns + sorted_definition_columns,
            ]
            plot_heatmap(
                sorted_correlation_matrix,
                f"{flavour.capitalize()} Flavour Correlation Matrix",
                os.path.join(flavour_dir, f"{flavour}_full_correlation_matrix.png"),
                {**filtered_label_mapping, **definition_label_mapping},
                threshold,
            )

            # Plot the filtered_names x filtered_names submatrix
            filtered_corr_matrix = correlation_matrix.loc[
                sorted_filtered_columns, sorted_filtered_columns
            ]
            plot_heatmap(
                filtered_corr_matrix,
                f"{flavour.capitalize()} Filtered Names Correlation Matrix",
                os.path.join(
                    flavour_dir, f"{flavour}_filtered_names_correlation_matrix.png"
                ),
                filtered_label_mapping,
                0.0,
            )

            # Plot the definition_dict x definition_dict submatrix
            definition_corr_matrix = correlation_matrix.loc[
                sorted_definition_columns, sorted_definition_columns
            ]
            plot_heatmap(
                definition_corr_matrix,
                f"{flavour.capitalize()} Definition Dict Correlation Matrix",
                os.path.join(
                    flavour_dir, f"{flavour}_definition_dict_correlation_matrix.png"
                ),
                definition_label_mapping,
                0.75,
            )

            # Plot the definition_dict x filtered_names submatrix
            def_filtered_corr_matrix = correlation_matrix.loc[
                sorted_definition_columns, sorted_filtered_columns
            ]
            plot_heatmap(
                def_filtered_corr_matrix,
                f"{flavour.capitalize()} Definition Dict x Filtered Names Correlation Matrix",
                os.path.join(
                    flavour_dir,
                    f"{flavour}_definition_dict_filtered_names_correlation_matrix.png",
                ),
                {**definition_label_mapping, **filtered_label_mapping},
                0.12,
            )

            # Plot the filtered_names x definition_dict submatrix
            filtered_def_corr_matrix = correlation_matrix.loc[
                sorted_filtered_columns, sorted_definition_columns
            ]
            plot_heatmap(
                filtered_def_corr_matrix,
                f"{flavour.capitalize()} Filtered Names x Definition Dict Correlation Matrix",
                os.path.join(
                    flavour_dir,
                    f"{flavour}_filtered_names_definition_dict_correlation_matrix.png",
                ),
                {**filtered_label_mapping, **definition_label_mapping},
                threshold,
            )
    elif split_region_B:
        for region, matrix in correlation_data.items():
            region_dir = os.path.join(output_dir, region)
            if not os.path.exists(region_dir):
                os.makedirs(region_dir, exist_ok=True)

            sorted_correlation_matrix = matrix.loc[
                sorted_filtered_columns + sorted_definition_columns,
                sorted_filtered_columns + sorted_definition_columns,
            ]
            plot_heatmap(
                sorted_correlation_matrix,
                f"Correlation Matrix for {region} DeepFlavB",
                os.path.join(region_dir, f"correlation_matrix_{region}.png"),
                {**filtered_label_mapping, **definition_label_mapping},
                threshold,
            )

            # Plot the filtered_names x filtered_names submatrix
            filtered_corr_matrix = matrix.loc[
                sorted_filtered_columns, sorted_filtered_columns
            ]
            plot_heatmap(
                filtered_corr_matrix,
                f"Filtered Names Correlation Matrix for {region} DeepFlavB",
                os.path.join(
                    region_dir, f"filtered_names_correlation_matrix_{region}.png"
                ),
                filtered_label_mapping,
                0.0,
            )

            # Plot the definition_dict x definition_dict submatrix
            definition_corr_matrix = matrix.loc[
                sorted_definition_columns, sorted_definition_columns
            ]
            plot_heatmap(
                definition_corr_matrix,
                f"Definition Dict Correlation Matrix for {region} DeepFlavB",
                os.path.join(
                    region_dir, f"definition_dict_correlation_matrix_{region}.png"
                ),
                definition_label_mapping,
                0.75,
            )

            # Plot the definition_dict x filtered_names submatrix
            def_filtered_corr_matrix = matrix.loc[
                sorted_definition_columns, sorted_filtered_columns
            ]
            plot_heatmap(
                def_filtered_corr_matrix,
                f"Definition Dict x Filtered Names Correlation Matrix for {region} DeepFlavB",
                os.path.join(
                    region_dir,
                    f"definition_dict_filtered_names_correlation_matrix_{region}.png",
                ),
                {**definition_label_mapping, **filtered_label_mapping},
                0.12,
            )

            # Plot the filtered_names x definition_dict submatrix
            filtered_def_corr_matrix = matrix.loc[
                sorted_filtered_columns, sorted_definition_columns
            ]
            plot_heatmap(
                filtered_def_corr_matrix,
                f"Filtered Names x Definition Dict Correlation Matrix for {region} DeepFlavB",
                os.path.join(
                    region_dir,
                    f"filtered_names_definition_dict_correlation_matrix_{region}.png",
                ),
                {**filtered_label_mapping, **definition_label_mapping},
                threshold,
            )

    else:
        # Plot the full correlation matrix
        sorted_correlation_matrix = correlation_data.loc[
            sorted_filtered_columns + sorted_definition_columns,
            sorted_filtered_columns + sorted_definition_columns,
        ]
        plot_heatmap(
            sorted_correlation_matrix,
            "Full Correlation Matrix",
            os.path.join(output_dir, "full_correlation_matrix.png"),
            {**filtered_label_mapping, **definition_label_mapping},
            threshold,
        )

        # Plot the filtered_names x filtered_names submatrix
        filtered_corr_matrix = correlation_data.loc[
            sorted_filtered_columns, sorted_filtered_columns
        ]
        plot_heatmap(
            filtered_corr_matrix,
            "Filtered Names Correlation Matrix",
            os.path.join(output_dir, "filtered_names_correlation_matrix.png"),
            filtered_label_mapping,
            0.0,
        )

        # Plot the definition_dict x definition_dict submatrix
        definition_corr_matrix = correlation_data.loc[
            sorted_definition_columns, sorted_definition_columns
        ]
        plot_heatmap(
            definition_corr_matrix,
            "Definition Dict Correlation Matrix",
            os.path.join(output_dir, "definition_dict_correlation_matrix.png"),
            definition_label_mapping,
            0.75,
        )

        # Plot the definition_dict x filtered_names submatrix
        def_filtered_corr_matrix = correlation_data.loc[
            sorted_definition_columns, sorted_filtered_columns
        ]
        plot_heatmap(
            def_filtered_corr_matrix,
            "Definition Dict x Filtered Names Correlation Matrix",
            os.path.join(
                output_dir, "definition_dict_filtered_names_correlation_matrix.png"
            ),
            {**definition_label_mapping, **filtered_label_mapping},
            0.12,
        )

        # Plot the filtered_names x definition_dict submatrix
        filtered_def_corr_matrix = correlation_data.loc[
            sorted_filtered_columns, sorted_definition_columns
        ]
        plot_heatmap(
            filtered_def_corr_matrix,
            "Filtered Names x Definition Dict Correlation Matrix",
            os.path.join(
                output_dir, "filtered_names_definition_dict_correlation_matrix.png"
            ),
            {**filtered_label_mapping, **definition_label_mapping},
            threshold,
        )


def main():
    parser = argparse.ArgumentParser(description="Generate correlation plots.")
    parser.add_argument(
        "folder", type=str, help="The folder containing the data files."
    )
    parser.add_argument(
        "--max_files",
        type=int,
        default=None,
        help="Maximum number of files to process.",
    )
    parser.add_argument(
        "--limit_inputs",
        action="store_true",
        help="Limit inputs to variables with npf, cpf, sv and _0",
    )
    parser.add_argument(
        "--limit_outputs",
        action="store_true",
        help="Limit outputs to names ending with B and not containing Neg",
    )
    parser.add_argument(
        "--SMu", action="store_true", help="Include branches with SMu in their name"
    )
    parser.add_argument(
        "--flavour_split",
        action="store_true",
        help="Split correlations by hadron flavour",
    )
    parser.add_argument(
        "--split_region_b", action="store_true", help="Split correlations by region B"
    )
    parser.add_argument(
        "--specify_MC",
        action="store_true",
        help="Specify MC file to make better data MC comparison",
    )
    args = parser.parse_args()

    data_file_paths = glob.glob(
        os.path.join(args.folder, "**", "*.root"), recursive=True
    )
    print(f"Data file paths: {data_file_paths}")  # Debugging line
    # print(data_file_paths)
    # print_definitions_keys()

    if data_file_paths:
        inspect_first_file(
            data_file_paths[0],
            limit_inputs=args.limit_inputs,
            limit_outputs=args.limit_outputs,
            SMu=args.SMu,
            flavour_split=args.flavour_split,
        )

    output_plot_path = os.path.join(args.folder, "output_plot.png")
    parquet_dir = os.path.join(args.folder, "parquet_data")

    data = load_data(
        data_file_paths,
        args.folder,
        max_files=args.max_files,
        limit_inputs=args.limit_inputs,
        limit_outputs=args.limit_outputs,
        SMu=args.SMu,
        flavour_split=args.flavour_split,
        specify_MC=args.specify_MC,
    )
    correlation_matrix, filtered_cols, def_cols = compute_correlations(
        data,
        SMu=args.SMu,
        flavour_split=args.flavour_split,
        split_region_B=args.split_region_b,
    )

    # Create a separate subfolder for plots if specify_MC is active
    if args.specify_MC:
        output_dir = os.path.join(args.folder, "correlation_plots_MC_specified")
    else:
        output_dir = os.path.join(args.folder, "correlation_plots")

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    plot_correlations(
        correlation_matrix,
        filtered_cols,
        def_cols,
        output_dir,
        flavour_split=args.flavour_split,
        split_region_B=args.split_region_b,
    )


if __name__ == "__main__":
    main()
